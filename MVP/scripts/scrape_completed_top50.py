#!/usr/bin/env python3
"""
Royal Road Best Rated å·²å®Œç»“ä¹¦ç±æŠ“å–è„šæœ¬
æŠ“å–å‰ 50 æœ¬å·²å®Œç»“ä¹¦ç±
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin
import re
import json

# User-Agent æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

BASE_URL = "https://www.royalroad.com"


def random_delay(min_sec=2, max_sec=4):
    """éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°"""
    delay = random.uniform(min_sec, max_sec)
    print(f"    â± ç­‰å¾… {delay:.1f} ç§’...")
    time.sleep(delay)


def get_soup(url, retry_count=3):
    """è·å–é¡µé¢å¹¶è¿”å› BeautifulSoup å¯¹è±¡"""
    for attempt in range(retry_count):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"    âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
            if attempt < retry_count - 1:
                time.sleep(random.uniform(3, 6))
            else:
                raise


def parse_number(text):
    """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œå¦‚ '1,234,567' -> 1234567"""
    if not text:
        return None
    # ç§»é™¤é€—å·å’Œå…¶ä»–éæ•°å­—å­—ç¬¦
    cleaned = re.sub(r'[^\d]', '', str(text))
    return int(cleaned) if cleaned else None


def extract_book_info(book_element):
    """ä»åˆ—è¡¨é¡µä¹¦ç±å…ƒç´ ä¸­æå–ä¿¡æ¯"""
    try:
        # è·å–é“¾æ¥å’Œæ ‡é¢˜ - æŸ¥æ‰¾ h2 æ ‡ç­¾
        title_elem = book_element.find('h2')
        if not title_elem:
            return None

        title_link = title_elem.find('a')
        if not title_link:
            return None

        link = title_link.get('href')
        full_url = urljoin(BASE_URL, link) if link else None
        title = title_link.get_text(strip=True)

        # å°é¢å›¾ - æŸ¥æ‰¾ img æ ‡ç­¾
        cover_img = book_element.find('img')
        cover_url = cover_img.get('src') if cover_img else None

        # ä»å…¨æ–‡ä¸­æå–ä¿¡æ¯
        all_text = book_element.get_text()

        # çŠ¶æ€ - COMPLETED, ONGOING, HIATUS, STUB
        status = "Unknown"
        for status_type in ["COMPLETED", "ONGOING", "HIATUS", "STUB"]:
            if status_type in all_text:
                status = status_type
                break

        # æå–æ ‡ç­¾ - ä»é“¾æ¥ä¸­æå–
        tags = []
        tag_links = book_element.find_all('a', href=lambda x: x and '/tags/' in str(x))
        for tag_elem in tag_links:
            tag_text = tag_elem.get_text(strip=True)
            # è¿‡æ»¤æ‰çŠ¶æ€æ ‡ç­¾ï¼ˆå¦‚ COMPLETEDï¼‰
            if tag_text and tag_text not in ["COMPLETED", "ONGOING", "HIATUS", "STUB", "Original", "Fan Fiction"]:
                if tag_text not in tags:
                    tags.append(tag_text)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
        if not tags:
            # å¸¸è§æ ‡ç­¾åˆ—è¡¨
            common_tags = [
                "Time Loop", "Adventure", "Fantasy", "Mystery", "Magic", "Comedy",
                "Sci-fi", "Action", "Slice of Life", "Romance", "LitRPG",
                "Reincarnation", "Portal Fantasy / Isekai", "Xianxia", "Urban Fantasy",
                "Super Heroes", "Female Lead", "Male Lead", "Villainous Lead",
                "Non-Human Lead", "Drama", "Horror", "High Fantasy", "Low Fantasy",
                "Space Opera", "Cyberpunk", "Dungeon", "Strategy", "Progression",
                "Virtual Reality", "GameLit", "Anti-Hero Lead", "Strong Lead"
            ]
            for tag in common_tags:
                if tag in all_text:
                    tags.append(tag)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç»Ÿè®¡ä¿¡æ¯
        # Followers
        followers_match = re.search(r'([\d,]+)\s*Followers?', all_text)
        followers = parse_number(followers_match.group(1)) if followers_match else None

        # Pages
        pages_match = re.search(r'([\d,]+)\s*Pages?', all_text)
        pages = parse_number(pages_match.group(1)) if pages_match else None

        # Views
        views_match = re.search(r'([\d,]+)\s*Views?', all_text)
        views = parse_number(views_match.group(1)) if views_match else None

        # Chapters
        chapters_match = re.search(r'([\d,]+)\s*Chapters?', all_text)
        chapters = parse_number(chapters_match.group(1)) if chapters_match else None

        # ç®€ä»‹ - è·å–æè¿°æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜åçš„æ–‡æœ¬ï¼‰
        # æŸ¥æ‰¾æ‰€æœ‰æ®µè½
        paragraphs = book_element.find_all('p')
        description = ""
        for p in paragraphs:
            text = p.get_text(strip=True)
            if text and len(text) > 50:  # åªå–è¾ƒé•¿çš„æ®µè½ä½œä¸ºç®€ä»‹
                description = text
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ®µè½ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not description:
            # è·å–æ•´ä¸ªæ–‡æœ¬å¹¶ç§»é™¤æ ‡é¢˜å’Œç»Ÿè®¡ä¿¡æ¯
            lines = all_text.split('\n')
            desc_lines = []
            for line in lines:
                line = line.strip()
                if line and len(line) > 30 and title not in line:
                    if not any(x in line for x in ['Followers', 'Pages', 'Views', 'Chapters', 'COMPLETED', 'ONGOING']):
                        desc_lines.append(line)
            description = ' '.join(desc_lines[:3])  # å–å‰3è¡Œ

        return {
            'title': title,
            'author': None,  # ä»è¯¦æƒ…é¡µè·å–
            'url': full_url,
            'coverUrl': cover_url,
            'status': status,
            'chapters': chapters,
            'pages': pages,
            'views': views,
            'followers': followers,
            'words': None,  # ä»è¯¦æƒ…é¡µè·å–
            'synopsis': description[:1000] if description else None,
            'platformRating': None,  # ä»è¯¦æƒ…é¡µè·å–
            'tags': ', '.join(tags[:10]) if tags else None  # é™åˆ¶æ ‡ç­¾æ•°é‡
        }
    except Exception as e:
        print(f"    âš ï¸ è§£æä¹¦ç±ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_book_details(url):
    """è·å–ä¹¦ç±è¯¦æƒ…é¡µä¿¡æ¯ï¼ˆä½œè€…ã€å­—æ•°ç­‰ï¼‰"""
    try:
        soup = get_soup(url)

        # ä½œè€…
        author = None
        author_link = soup.find('a', href=lambda x: x and '/profile/' in x)
        if author_link:
            author = author_link.get_text(strip=True)

        # å­—æ•° - é€šå¸¸åœ¨ç»Ÿè®¡ä¿¡æ¯ä¸­
        words = None
        stats_section = soup.find('div', class_='fiction-stats')
        if stats_section:
            stats_text = stats_section.get_text()
            words_match = re.search(r'([\d,]+)\s*Words?', stats_text)
            if words_match:
                words = parse_number(words_match.group(1))

        # è¯„åˆ† - ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•
        rating = None

        # æ–¹æ³•1: ä» meta æ ‡ç­¾æå–
        meta_rating = soup.find('meta', property='books:rating:value')
        if meta_rating and meta_rating.get('content'):
            try:
                rating = float(meta_rating['content'])
            except:
                pass

        # æ–¹æ³•2: ä» JSON-LD æå–
        if not rating:
            json_ld = soup.find('script', type='application/ld+json')
            if json_ld:
                try:
                    data = json.loads(json_ld.string)
                    if 'aggregateRating' in data:
                        rating = float(data['aggregateRating']['ratingValue'])
                except:
                    pass

        # æ–¹æ³•3: ä» HTML æ–‡æœ¬ä¸­æŸ¥æ‰¾
        if not rating:
            pattern = r'books:rating:value"\s+content="(\d+\.\d+)"'
            match = re.search(pattern, str(soup))
            if match:
                rating = float(match.group(1))

        return {
            'author': author,
            'words': words,
            'platformRating': rating
        }
    except Exception as e:
        print(f"    âš ï¸ è·å–è¯¦æƒ…é¡µå‡ºé”™: {e}")
        return {}


def scrape_bestRated(pages=15, target_count=50):
    """æŠ“å– Best Rated æ¦œå•ä¸­å·²å®Œç»“çš„ä¹¦ï¼ˆåŒ…æ‹¬STUBBEDï¼‰"""
    print(f"ğŸš€ å¼€å§‹æŠ“å– Royal Road Best Rated æ¦œå•ä¸­å·²å®Œç»“çš„å‰ {target_count} æœ¬")
    print("=" * 60)

    all_books = []
    seen_urls = set()  # ç”¨äºå»é‡

    for page in range(1, pages + 1):
        if len(all_books) >= target_count:
            break

        print(f"\nğŸ“– æ­£åœ¨æŠ“å–ç¬¬ {page}/{pages} é¡µ...")

        url = f"{BASE_URL}/fictions/best-rated?page={page}"
        soup = get_soup(url)

        # æŸ¥æ‰¾æ‰€æœ‰å°è¯´æ¡ç›® - Royal Road ä½¿ç”¨ fiction-card ç±»
        book_elements = soup.find_all('div', class_='fiction-card')

        if not book_elements:
            # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
            book_elements = soup.find_all('div', class_='row')
            book_elements = [elem for elem in book_elements if elem.find('h2')]

        if not book_elements:
            # å†å°è¯•å…¶ä»–é€‰æ‹©å™¨
            book_elements = soup.find_all('article')

        print(f"    ğŸ“š æ‰¾åˆ° {len(book_elements)} æœ¬ä¹¦")

        for idx, book_elem in enumerate(book_elements, 1):
            if len(all_books) >= target_count:
                break

            # å…ˆæ£€æŸ¥çŠ¶æ€ - åŒ…æ‹¬ COMPLETED å’Œ STUBBED
            all_text = book_elem.get_text()
            is_completed = "COMPLETED" in all_text
            is_stubbed = "STUBBED" in all_text

            # è·³è¿‡æœªå®Œç»“çš„ä¹¦
            if not (is_completed or is_stubbed):
                continue

            print(f"    [{len(all_books) + 1}/{target_count}] ", end="")

            book_info = extract_book_info(book_elem)

            if book_info and book_info['url']:
                # æ£€æŸ¥æ˜¯å¦å·²æŠ“å–è¿‡
                if book_info['url'] in seen_urls:
                    print(f"âŠ˜ {book_info['title'][:30]}... [è·³è¿‡é‡å¤]")
                    continue

                seen_urls.add(book_info['url'])

                status_text = "å·²å®Œç»“" if is_completed else "å·²å®Œç»“(Stub)"
                print(f"âœ“ {book_info['title'][:30]}... [{status_text}]")

                # è·å–è¯¦æƒ…é¡µä¿¡æ¯
                try:
                    random_delay(1, 2)  # è¯¦æƒ…é¡µå»¶è¿Ÿç¨çŸ­
                    details = get_book_details(book_info['url'])
                    book_info.update(details)

                    # æ˜¾ç¤ºè¯„åˆ†
                    if book_info.get('platformRating'):
                        print(f"       â­ è¯„åˆ†: {book_info['platformRating']}")
                except Exception as e:
                    print(f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: {e}")

                all_books.append(book_info)
            else:
                print("âœ— è·³è¿‡")

        # é¡µé¢ä¹‹é—´å»¶è¿Ÿ
        if page < pages and len(all_books) < target_count:
            random_delay()

    print("\n" + "=" * 60)
    print(f"âœ… æŠ“å–å®Œæˆï¼å…±è·å– {len(all_books)} æœ¬å·²å®Œç»“ä¹¦ç±ï¼ˆåŒ…æ‹¬STUBBEDï¼‰")

    return all_books


def save_to_excel(books, filename="rr_best_rated.xlsx"):
    """ä¿å­˜åˆ° Excel æ–‡ä»¶"""
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆ° {filename}...")

    df = pd.DataFrame(books)

    # è°ƒæ•´åˆ—é¡ºåº
    columns_order = [
        'title', 'author', 'url', 'coverUrl',
        'status', 'chapters', 'pages', 'words',
        'views', 'followers', 'synopsis',
        'platformRating', 'tags'
    ]

    # åªä¿ç•™å­˜åœ¨çš„åˆ—
    columns_order = [col for col in columns_order if col in df.columns]
    df = df[columns_order]

    df.to_excel(filename, index=False, engine='openpyxl')
    print(f"âœ… ä¿å­˜æˆåŠŸï¼æ–‡ä»¶: {filename}")
    print(f"\nğŸ“Š æ•°æ®é¢„è§ˆ:")
    print(df.head(3).to_string())


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æŠ“å–æ•°æ® - å·²å®Œç»“çš„å‰50æœ¬ï¼ˆæœ€å¤š15é¡µï¼‰
        books = scrape_bestRated(pages=15, target_count=50)

        if books:
            # ä¿å­˜åˆ° Excel
            save_to_excel(books, filename="rr_completed_top50.xlsx")
        else:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

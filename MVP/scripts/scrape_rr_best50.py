#!/usr/bin/env python3
"""
Royal Road Best Rated æ¦œå•æŠ“å–è„šæœ¬ï¼ˆå¸¦ Show Stubsï¼‰
æŠ“å–å‰ 50 æœ¬ä¹¦ï¼ˆçº¦ 2-3 é¡µï¼‰
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from urllib.parse import urljoin
import re
import json

# User-Agent æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

BASE_URL = "https://www.royalroad.com"


def random_delay(min_sec=3, max_sec=6):
    """éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°"""
    delay = random.uniform(min_sec, max_sec)
    print(f"    â± ç­‰å¾… {delay:.1f} ç§’...")
    time.sleep(delay)


def get_soup(url, retry_count=3):
    """è·å–é¡µé¢å¹¶è¿”å› BeautifulSoup å¯¹è±¡"""
    for attempt in range(retry_count):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"    âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{retry_count}): {e}")
            if attempt < retry_count - 1:
                time.sleep(random.uniform(3, 6))
            else:
                raise


def parse_number(text):
    """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œå¦‚ '1,234,567' -> 1234567"""
    if not text:
        return None
    cleaned = re.sub(r'[^\d]', '', str(text))
    return int(cleaned) if cleaned else None


def get_book_rating(soup):
    """ä»è¯¦æƒ…é¡µè·å–è¯„åˆ†"""
    rating = None

    # æ–¹æ³•1: ä» meta æ ‡ç­¾æå–
    meta_rating = soup.find('meta', property='books:rating:value')
    if meta_rating and meta_rating.get('content'):
        try:
            rating = float(meta_rating['content'])
        except:
            pass

    # æ–¹æ³•2: ä» JSON-LD æå–
    if not rating:
        json_ld = soup.find('script', type='application/ld+json')
        if json_ld:
            try:
                data = json.loads(json_ld.string)
                if 'aggregateRating' in data:
                    rating = float(data['aggregateRating']['ratingValue'])
            except:
                pass

    # æ–¹æ³•3: ä» HTML æ–‡æœ¬ä¸­æŸ¥æ‰¾
    if not rating:
        pattern = r'books:rating:value"\s+content="(\d+\.\d+)"'
        match = re.search(pattern, str(soup))
        if match:
            rating = float(match.group(1))

    return rating


def extract_book_info(book_element):
    """ä»åˆ—è¡¨é¡µä¹¦ç±å…ƒç´ ä¸­æå–ä¿¡æ¯"""
    try:
        # è·å–é“¾æ¥å’Œæ ‡é¢˜ - æŸ¥æ‰¾ h2 æ ‡ç­¾
        title_elem = book_element.find('h2')
        if not title_elem:
            return None

        title_link = title_elem.find('a')
        if not title_link:
            return None

        link = title_link.get('href')
        full_url = urljoin(BASE_URL, link) if link else None
        title = title_link.get_text(strip=True)

        # å°é¢å›¾ - æŸ¥æ‰¾ img æ ‡ç­¾
        cover_img = book_element.find('img')
        cover_url = cover_img.get('src') if cover_img else None

        # ä»å…¨æ–‡ä¸­æå–ä¿¡æ¯
        all_text = book_element.get_text()

        # çŠ¶æ€ - COMPLETED, ONGOING, HIATUS, STUB
        status = "Unknown"
        for status_type in ["COMPLETED", "ONGOING", "HIATUS", "STUB"]:
            if status_type in all_text:
                status = status_type
                break

        # æå–æ ‡ç­¾ - ä»é“¾æ¥ä¸­æå–
        tags = []
        tag_links = book_element.find_all('a', href=lambda x: x and '/tags/' in str(x))
        for tag_elem in tag_links:
            tag_text = tag_elem.get_text(strip=True)
            # è¿‡æ»¤æ‰çŠ¶æ€æ ‡ç­¾
            if tag_text and tag_text not in ["COMPLETED", "ONGOING", "HIATUS", "STUB", "Original", "Fan Fiction"]:
                if tag_text not in tags:
                    tags.append(tag_text)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç»Ÿè®¡ä¿¡æ¯
        followers_match = re.search(r'([\d,]+)\s*Followers?', all_text)
        followers = parse_number(followers_match.group(1)) if followers_match else None

        pages_match = re.search(r'([\d,]+)\s*Pages?', all_text)
        pages = parse_number(pages_match.group(1)) if pages_match else None

        views_match = re.search(r'([\d,]+)\s*Views?', all_text)
        views = parse_number(views_match.group(1)) if views_match else None

        chapters_match = re.search(r'([\d,]+)\s*Chapters?', all_text)
        chapters = parse_number(chapters_match.group(1)) if chapters_match else None

        # ç®€ä»‹ - è·å–æè¿°æ–‡æœ¬
        paragraphs = book_element.find_all('p')
        description = ""
        for p in paragraphs:
            text = p.get_text(strip=True)
            if text and len(text) > 50:
                description = text
                break

        if not description:
            lines = all_text.split('\n')
            desc_lines = []
            for line in lines:
                line = line.strip()
                if line and len(line) > 30 and title not in line:
                    if not any(x in line for x in ['Followers', 'Pages', 'Views', 'Chapters', 'COMPLETED', 'ONGOING']):
                        desc_lines.append(line)
            description = ' '.join(desc_lines[:3])

        return {
            'title': title,
            'author': None,
            'url': full_url,
            'coverUrl': cover_url,
            'status': status,
            'chapters': chapters,
            'pages': pages,
            'views': views,
            'followers': followers,
            'words': None,
            'synopsis': description[:1000] if description else None,
            'platformRating': None,
            'tags': ', '.join(tags[:10]) if tags else None
        }
    except Exception as e:
        print(f"    âš ï¸ è§£æä¹¦ç±ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_book_details(url):
    """è·å–ä¹¦ç±è¯¦æƒ…é¡µä¿¡æ¯ï¼ˆä½œè€…ã€å­—æ•°ã€è¯„åˆ†ç­‰ï¼‰"""
    try:
        soup = get_soup(url)

        # ä½œè€…
        author = None
        author_link = soup.find('a', href=lambda x: x and '/profile/' in x)
        if author_link:
            author = author_link.get_text(strip=True)

        # å­—æ•°
        words = None
        stats_section = soup.find('div', class_='fiction-stats')
        if stats_section:
            stats_text = stats_section.get_text()
            words_match = re.search(r'([\d,]+)\s*Words?', stats_text)
            if words_match:
                words = parse_number(words_match.group(1))

        # è¯„åˆ†
        rating = get_book_rating(soup)

        return {
            'author': author,
            'words': words,
            'platformRating': rating
        }
    except Exception as e:
        print(f"    âš ï¸ è·å–è¯¦æƒ…é¡µå‡ºé”™: {e}")
        return {}


def scrape_bestRated_completed(target_count=50):
    """æŠ“å– Best Rated æ¦œå•ä¸­å·²å®Œæˆï¼ˆCOMPLETEDï¼‰çŠ¶æ€çš„å‰50æœ¬"""
    print(f"ğŸš€ å¼€å§‹æŠ“å– Royal Road Best Rated æ¦œå•ä¸­å·²å®Œç»“çš„å‰ {target_count} æœ¬")
    print("=" * 80)

    all_books = []
    page = 1

    while len(all_books) < target_count and page <= 10:  # æœ€å¤šæŠ“10é¡µ
        print(f"\nğŸ“– æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ...")

        url = f"{BASE_URL}/fictions/best-rated?page={page}"

        try:
            soup = get_soup(url)

            # æŸ¥æ‰¾æ‰€æœ‰å°è¯´æ¡ç›® - å°è¯•å¤šç§é€‰æ‹©å™¨
            book_elements = soup.find_all('div', class_='fiction-card')

            if not book_elements:
                # å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å« h2 çš„ div
                book_elements = []
                all_divs = soup.find_all('div')
                for div in all_divs:
                    if div.find('h2'):
                        # ç¡®ä¿è¿™ä¸ª div åŒ…å«å°è¯´é“¾æ¥
                        links = div.find_all('a', href=lambda x: x and '/fiction/' in str(x))
                        if links:
                            book_elements.append(div)
                            # ç§»é™¤å·²ä½¿ç”¨çš„ div çš„å­å…ƒç´ ï¼Œé¿å…é‡å¤
                            for sub_div in div.find_all('div'):
                                if sub_div in all_divs:
                                    all_divs.remove(sub_div)

            if not book_elements:
                print(f"    âš ï¸ æœªæ‰¾åˆ°ä¹¦ç±å…ƒç´ ")
                break

            print(f"    ğŸ“š æ‰¾åˆ° {len(book_elements)} æœ¬ä¹¦")

            for idx, book_elem in enumerate(book_elements, 1):
                if len(all_books) >= target_count:
                    break

                # å…ˆæå–åŸºæœ¬ä¿¡æ¯æ£€æŸ¥çŠ¶æ€
                title_elem = book_elem.find('h2')
                if not title_elem:
                    continue

                title_link = title_elem.find('a')
                if not title_link:
                    continue

                link = title_link.get('href')
                full_url = urljoin(BASE_URL, link) if link else None

                # æ£€æŸ¥çŠ¶æ€
                all_text = book_elem.get_text()
                is_completed = "COMPLETED" in all_text

                # åªæŠ“å–å·²å®Œæˆçš„ä¹¦ç±
                if not is_completed:
                    continue

                print(f"    [{len(all_books) + 1}/{target_count}] ", end="")

                book_info = extract_book_info(book_elem)

                if book_info and book_info['url']:
                    print(f"âœ“ {book_info['title'][:30]}... [å·²å®Œæˆ]")

                    # è·å–è¯¦æƒ…é¡µä¿¡æ¯
                    try:
                        random_delay(1, 2)
                        details = get_book_details(book_info['url'])
                        book_info.update(details)

                        # æ˜¾ç¤ºè¯„åˆ†
                        if book_info.get('platformRating'):
                            print(f"       â­ è¯„åˆ†: {book_info['platformRating']}")
                    except Exception as e:
                        print(f"      âš ï¸ è·å–è¯¦æƒ…å¤±è´¥: {e}")

                    all_books.append(book_info)
                else:
                    print("âœ— è·³è¿‡")

        except Exception as e:
            print(f"    âŒ ç¬¬ {page} é¡µæŠ“å–å¤±è´¥: {e}")
            break

        # é¡µé¢ä¹‹é—´å»¶è¿Ÿ
        page += 1
        if len(all_books) < target_count and page <= 10:
            random_delay()

    print("\n" + "=" * 80)
    print(f"âœ… æŠ“å–å®Œæˆï¼å…±è·å– {len(all_books)} æœ¬å·²å®Œç»“ä¹¦ç±")

    return all_books


def save_to_excel(books, filename="rr_best50_with_stubs.xlsx"):
    """ä¿å­˜åˆ° Excel æ–‡ä»¶"""
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åˆ° {filename}...")

    df = pd.DataFrame(books)

    # è°ƒæ•´åˆ—é¡ºåº
    columns_order = [
        'title', 'author', 'url', 'coverUrl',
        'platformRating',
        'status', 'chapters', 'pages', 'words',
        'views', 'followers', 'synopsis',
        'tags'
    ]

    # åªä¿ç•™å­˜åœ¨çš„åˆ—
    columns_order = [col for col in columns_order if col in df.columns]
    df = df[columns_order]

    df.to_excel(filename, index=False, engine='openpyxl')
    print(f"âœ… ä¿å­˜æˆåŠŸï¼æ–‡ä»¶: {filename}")

    # æ˜¾ç¤ºé¢„è§ˆ
    print(f"\nğŸ“Š æ•°æ®é¢„è§ˆ:")
    print(df.head(10).to_string())


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æŠ“å–æ•°æ® - åªæŠ“å–å·²å®Œç»“çš„ä¹¦
        books = scrape_bestRated_completed(target_count=50)

        if books:
            # ä¿å­˜åˆ° Excel
            save_to_excel(books, filename="rr_best50_completed.xlsx")

            # æ˜¾ç¤ºç»Ÿè®¡
            print("\n" + "=" * 80)
            print("ğŸ“Š æŠ“å–ç»Ÿè®¡:")
            print(f"   æ€»ä¹¦ç±æ•°: {len(books)}")
            print(f"   å·²å®Œç»“ä¹¦ç±: {len(books)}")

            if 'platformRating' in pd.DataFrame(books).columns:
                df = pd.DataFrame(books)
                has_rating = df['platformRating'].notna().sum()
                print(f"   æœ‰è¯„åˆ†çš„ä¹¦: {has_rating}")
                if has_rating > 0:
                    print(f"   æœ€é«˜è¯„åˆ†: {df['platformRating'].max():.2f}")
                    print(f"   æœ€ä½è¯„åˆ†: {df['platformRating'].min():.2f}")
                    print(f"   å¹³å‡è¯„åˆ†: {df['platformRating'].mean():.2f}")
        else:
            print("âŒ æ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
